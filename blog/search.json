[
  {
    "objectID": "2016/08/27/turing-complete.html",
    "href": "2016/08/27/turing-complete.html",
    "title": "Turing Completeness",
    "section": "",
    "text": "This is one of my pet peeves: I have heard many people describe Turing-complete as “a language you can do everything with”. That is untrue and this is a very short post to set this right.\nTuring-complete means a language can be used to express a (probably non-proper – if you manage to find a system that is more expressible than the Turing machine, I am sure there is a Turing award or two waiting for you) superset of the pure mathematical functions that a Turing machine can compute. This does not talk about I/O (which is an important part of an actual computer) or the complexity these functions are executed in. So, I am afraid we will not see an operating system written in the Game of Life.\nThis definition also means an actual computer is technically not Turing complete, given it cannot simulate a Turing machine’s infinite tape.\nSo, concluding: Turing-complete does not mean a language is useful to do something on an actual computer, while an actual computer is not actually Turing-complete. The two concepts are quite orthogonal.\nElaborating more (it was late yesterday): I/O can be “simulated” with appropriate encoding in the output, but that is one of the points: arithmetic is Turing-complete, so I can, for all Turing-computable functions, produce an integer that somehow encodes its result. This is an extremely important theoretical construct in computation theory, but also useless in a practical context, because I want to write actual data into actual memory of an actual machine."
  },
  {
    "objectID": "2016/09/03/a-paper-a-week-keystroke-recognition-wifi.html",
    "href": "2016/09/03/a-paper-a-week-keystroke-recognition-wifi.html",
    "title": "A Paper a Whatever #15: Keystroke Recognition Using WiFi Signals",
    "section": "",
    "text": "This paper is about detecting a user’s keypresses using the interference the movements of the fingers cause in WiFi signals. This sounds scary and like something out of a cheesy spy movie; nevertheless it is not quite time to get out your tinfoil hat, as (at least as demonstrated) this is only accurate in controlled conditions where equipment is specifically set-up and the user is instructed not to move their head. It does not seem like there is anything that makes it inherently impossible to do in uncontrolled conditions.\nThe introduction talks about various other interesting approaches other people have taken to recognize keystrokes, including matching on the distinctive sound different keypresses produce or their electromagnetic signature. There is also prior work in using radio interference patterns, but the authors of this paper took the different approach of only using over-the-counter equipment for doing so. I was not aware at all that there was this much research going on on essentially remote key-logging.\nTheir approach, however, exploits something known as CSI values – sadly, no, that does not stand for Crime Scene Investigation. CSI stands for Channel State Information and is a set of values that WiFi devices use to optimize their transmission among multiple channels.\nThey observed that it is possible to find signatures for different movements of the fingers in those CSI values, but they faced the problem that they needed to determine when a key-press started and ended, and noise was also a problem. They would denoise the signal by first applying low-pass filtering and then then applying Principal Component Analysis on the data retrieved from the CSI of various channels – they noticed that while the changes were not the same in all of them, they were strongly correlated. So they applied PCA, discarding the highest variance component.\nMean Absolute Deviations are then used to find parts of the timeseries that are high in variance and correspond to keypresses by comparing them to empirically obtained thresholds. The thus found timeseries are then used to train a k Nearest Neighbour model, which would classify each unknown sample with the class that the majority of its k nearest neighbours from the training data-set correspond to. They use Dynamic Time Warping as a distance metric, which tries to align two data-sets by non-linearly warping the time axis until they match. This allows them to compare the signatures of keypresses, even if they are pressed longer or shorter than in the training data.\nThis model is then trained with 30 data-samples per user per key, and numbers for accuracy are given. This is with the caveat that the experiment was done under controlled conditions, in particular the users were asked to type one key at a time and not to move their heads or other body parts. Training the model independently for each user, they managed to achieve 93.5 % keypress recognition accuracy in continuously typed sentences.\nFind the paper here."
  },
  {
    "objectID": "2016/11/23/someone-is-wrong-on-the-internet.html",
    "href": "2016/11/23/someone-is-wrong-on-the-internet.html",
    "title": "More Turing Completeness, or: Somebody is wrong on the Internet",
    "section": "",
    "text": "Hello, I am Florian and you might remember me from posts such as Turing Completeness. Today I am going to return to the topic of Turing Completeness, in a post in similar vein. Today’s post explores the arguments Zed Shaw presents in Learn Python The Hard Way on why Python 3 is the inferior version of the language; in particular, how the concept of Turing Completeness is abused to make a point that is completely unrelated to it. Note that I explicitly do not want to take any position in the Python 2 vs. Python 3 argument, I merely want to expose the flawed logic in this particular line of reasoning.\nLet’s go explore the argument presented and why it is flawed.\n\nPython 3 Is Not Turing Complete\n\nWith the right definition of Python 3, this might actually be technically correct. Any computer language executed on a real physical computer can never be truly Turing complete. Because all resources in a computer are finite, you could theoretically enumerate all possible states. If you have studied a bit of computation theory, Turing Machines are more powerful than that (also, you will know that functions that you can enumerate are usually “boring” from a computation theory point of view).\n\nIn computer science a fundamental law is that if I have one Turing Machine I can build any other Turing Machine.\n\nI am not completely sure what that even means. If you have a Turing Machine that just does nothing, I can not use that to build any Turing Machine. There is a Turing Machine (also known as the Universal Turing Machine) that can simulate all other Turing machines. Note that this requires the Turing Machines it simulates to be encoded in some suitable way.\n\nIf I have COBOL then I can bootstrap a compiler for FORTRAN (as disgusting as that might be). If I have FORTH, then I can build an interpreter for Ruby.\n\nIf you have Python 3, you can definitely use that to write an interpreter of Python 2 in it, if you really really want to. But even that has no direct correspondence to the point of Turing Completeness. Note how this talks about specific encodings of a program (i.e. programming languages), while the concept of Turing Completeness concerns itself with mathematical (partial) functions.\nA mathematical function is just a mapping from input to output, while a program is a procedure to produce said output. It is completely possible that a Turing Complete system is unable to express a program another one can. The Game of Life is Turing Complete, but it will definitely will never be able to parse your Python program from your hard-disk.\n\nCurrently you cannot run Python 2 inside the Python 3 virtual machine. Since I cannot, that means Python 3 is not Turing Complete and should not be used by anyone.\n\nNo, no. It really really does not mean either of these things.\n\nNote\nYes, that is kind of funny way of saying that there’s no reason why Python 2 and Python 3 can’t coexist other than the Python project’s incompetence and arrogance. Obviously it’s theoretically possible to run Python 2 in Python 3, but until they do it then they have decided to say that Python 3 cannot run one other Turing complete language so logically Python 3 is not Turing complete. I should also mention that as stupid as that sounds, actual Python project developers have told me this, so it’s their position that their own language is not Turing complete.\n\nI would really like to see these conversations. Either these Python developers are misquoted here, or were wrong (or, as explored above: technically correct – but that has nothing to do with any of the arguments that were made).\nThe rest of the argument goes on in similar fashion, incorrectly combining concepts of theoretical computer science with implementations, citing how other virtual machines manage to be more general purpose, and so on, and so forth; the latter of which, of course, is true – but invariantly true for CPython 2 and CPython 3."
  },
  {
    "objectID": "2016/04/03/theorem-proving-logic-puzzles.html",
    "href": "2016/04/03/theorem-proving-logic-puzzles.html",
    "title": "Using theorem proving to cheat in logic puzzles",
    "section": "",
    "text": "I recently got the book What is the Name of this Book? by the excellent Raymon Smullyan, who is also the author of a book about Gödel’s Incompleteness Theorems I could not praise highly enough. The book I purchased is a collection of logical riddles. While recreationally solving logical puzzles oneself can be very rewarding, when I got the book I considered that it would be interesting to try and solve these using formalized logic. Letting a computer find the solution given the formal specification is the obvious next step, which is what I will attempt in this blogpost. To do so, I will first formalize the specification in first-order logic, and then use the Z3 SMT solver to find satisfying models.\nA lot of the book is about a world in which there are two types of people, knights and knaves. Knights always speak the truth, and knaves always lie. This particular puzzle is about finding out information about the one werewolf among people A, B and C that make the following statements.\n\nA: C is a werewolf.\nB: I am not a werewolf.\nC: At least two of us are knaves.\n\nWe are trying to find answers to the following two questions:\n\nIs the werewolf a knight or a knave?\nIf you have to take one of them as a travelling companion, and it is more important that he be not a werewolf than that he not be a knave, which one would you pick?\n\nFirst we will formalize the fact that there is exactly one werewolf as the two statements “there exists at least one werewolf” and “the werewolf is unique” (we assume that the universe contains only persons, because they are the only objects in this riddle, so we do not need a predicate for “is person”):\n\\[\n\\exists x\\, Werewolf(x)\n\\tag{1}\\]\n\\[\n\\forall x\\, (Werewolf(x) \\implies (\\forall y\\, (x \\ne y \\implies \\neg Werewolf(y))))\n\\tag{2}\\]\nThen, we introduce the Knight predicate (and an equivalence of negative Knight to Knave for convenience)\n\\[\n\\forall x\\, (Knight(x) \\equiv \\neg Knave(x))\n\\tag{3}\\]\nNext we can formalize the three statements, which are true if and only if the person that says them is a Knight.\n\\[\nKnight(A) \\equiv Werewolf(C)\n\\tag{4}\\]\n\\[\nKnight(B) \\equiv \\neg Werewolf(B)\n\\tag{5}\\]\n\\[\nKnight(C) \\equiv \\exists x\\, \\exists y\\, (x \\ne y \\land Knave(x) \\land Knave(y))\n\\tag{6}\\]\nIf we call the set of equations (Equation 1) to (Equation 6) \\(\\Pi\\), then we want to find out whether\n\\[\n\\Pi \\models \\exists x\\, (Knight(x) \\land Werewolf(x))\n\\tag{7}\\]\n\\[\n\\Pi \\models \\exists x\\, (Knave(x) \\land Werewolf(x))\n\\tag{8}\\]\nThen, to answer the second question, we want to check\n\\[\n\\Pi \\models \\neg Werewolf(A)\n\\tag{9}\\]\n\\[\n\\Pi \\models \\neg Werewolf(B)\n\\tag{10}\\]\n\\[\n\\Pi \\models \\neg Werewolf(C)\n\\tag{11}\\]\nSo, with that out of the way, let us introduce the tool that will make sure we do not need to do any thinking: the Z3 SMT solver. SMT stands for satisfyability modulo theory and means that you can give it first-order formulas of some theories (like equality, integers, computer arithmetic, …) and it will try to find a satisfying model. Given infinite domains there is no guarantee that will be found (especially the negative answer is particularly challenging – if there is a positive one it will be found by enumeration eventually). If you want to prove \\(\\Pi \\models A\\) using an SMT solver, in general what you do is apply the deduction theorem to obtain \\(\\models \\pi \\implies A\\) (where \\(\\pi\\) is the conjunction of the elements of \\(\\Pi\\)). All possible models satisfy this if and only if there is no model satisfying the negation, i.e. \\(\\neg ( \\pi \\implies A)\\), which can be rewritten as \\(\\pi \\land \\neg A\\). In other words, we ask the SMT solver if \\(\\pi \\land \\neg A\\) is satisfyable, and if the answer is no, \\(\\Pi \\models A\\) holds.\nNow all that’s left is translating this specification to Z3’s language. First thing we need to do is define a datatype for our objects, that are Persons. By using declare-datatype we get objects that are only equal to themselves, so we can use the equality predicate as above.\n(set-option :timeout 600)\n(declare-datatypes () ((Person A B C)))\nThen, we need to define our three predicates Knight, Knave and Werewolf.\n(declare-fun Knight (Person) Bool)\n(declare-fun Knave (Person) Bool)\n(declare-fun Werewolf (Person) Bool)\nThen we can formalize the formulas of \\(\\Pi\\) into Z3 syntax pretty much verbatim. The statement that there is exactly one werewolf can be expressed with the two assertions that a model has to satisfy\n(assert (exists ((x Person)) (Werewolf x)))\n(assert (forall ((x Person)) (implies (Werewolf x) (forall ((y Person)) (implies (not (= x y)) (not (Werewolf y)))))))\nThe three statements by A, B and C become the following. Note that = is used for both logical equivalence and equality between objects of the universe in Z3.\n(assert (forall ((x Person)) (= (Knight x) (not (Knave x)))))\n(assert (= (Knight A) (Werewolf C)))\n(assert (= (Knight B) (not (Werewolf B))))\n(assert (= (Knight C) (exists ((x Person) (y Person)) (and (not (= x y)) (and (Knave x) (Knave y))))))\nNow we have asserted all we know about the problem, we can begin trying to prove (Equation 7) – (Equation 11) to see which one the solutions is correct. We use the push and pop operations of Z3 to temporarily add add assertions between the two operations. I simplified \\(\\neg \\neg Werewolf(…)\\) to \\(Werewolf(…)\\)\n(push)\n(assert (not (exists ((x Person)) (and (Werewolf x) (Knave x)))))\n(check-sat)\n(pop)\n\n(push)\n(assert (not (exists ((x Person)) (and (Werewolf x) (Knight x)))))\n(check-sat)\n(pop)\n\n(push)\n(assert (Werewolf A))\n(check-sat)\n(pop)\n\n(push)\n(assert (Werewolf B))\n(check-sat)\n(pop)\n\n(push)\n(assert (Werewolf C))\n(check-sat)\n(pop)\nZ3’s answer to the input we gave it is\nunsat\nsat\nunsat\nsat\nsat\nThis means that (Equation 7) and (Equation 9) hold in all models, and the others do not. This means that the werewolf is a Knave, and we should take person A as our travelling companion.\nWhile a rather simple example, I think this is a nice demonstration of what one can do with an SMT solver."
  },
  {
    "objectID": "2020/04/19/linux-the-good-the-bad-and-the-ugly.html",
    "href": "2020/04/19/linux-the-good-the-bad-and-the-ugly.html",
    "title": "Linux: The good, the bad and the ugly",
    "section": "",
    "text": "I’ve been working on system-level software for Linux for some time now, which is a job that makes you much more likely to encounter the odd corner of an operating system than higher level development. For all practical purposes, Linux has succeeded as an operating system. Billions of web queries are being served from Linux servers every day, and it’s running on billions of mobile devices.\nBut of course an operating system ultimately rooted in the 70s will have accumulated its fair share of legacy. Of course, not all problems are rooted in this: we still are perfectly capable of making new and exciting mistakes."
  },
  {
    "objectID": "2020/04/19/linux-the-good-the-bad-and-the-ugly.html#footnotes",
    "href": "2020/04/19/linux-the-good-the-bad-and-the-ugly.html#footnotes",
    "title": "Linux: The good, the bad and the ugly",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOr cheat, I’m a sign, not a cop.↩︎\nGNU/Linux, FreeBSD, or macOS.↩︎\nSpoilers for Ted Chiang’s story “Anxiety Is the Dizziness of Freedom”. It’s like the premise of that story. (Assuming single-threaded) fork brings forth a new process that is the same, except for one int, which is used to make a choice. All other differences derive from that.↩︎\nI’m not a fan of syscalls without wrappers, as that encourages people to use raw syscalls, which then constrains libc implementors. clone is actually a good example: Bionic (Android’s libc) has a cache for the TID of a thread. It needs to invalidate that cache upon calls to fork / clone / etc. It can only do this when people use the syscall wrapper. People don’t always.↩︎\nSome of the more low-level details leave a lot to be desired. Many things are impossible in a standards compliant way, or are underspecified, so people end up doing what works in practice.↩︎\nThis, of course, leads to the question of what constitutes a feature, and what constitutes a bug. And what level of quality you expect from user-space. For instance, adding a new line to a proc file would break applications that do not properly look at key, value pairs, but rather hard-code the line they expect the value to be at. It’s generally accepted that writing code like this is asking to be broken.↩︎\nThis has actually happened to me before. But if you know where to look (man execve), it’s at least documented.↩︎\nI do have opinions on more things, including but not limited to: perf_event_open, signals, sendmsg, session leaders, controlling terminals, SIGPIPE, …↩︎\nDepends on whether your kernel has this commit.↩︎\nThe manpage uses spaces.↩︎"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is the blog of Florian Mayer. I am a Software Engineer at Google UK. All views expressed here are mine alone and neither those of Google nor of the Queen of England.\nBefore I joined Google, I studied Software & Information Engineering at Vienna University of Technology, followed by a semester of Computational Intelligence. I am very interested in formal logic and theoretical computer science."
  },
  {
    "objectID": "2015/12/02/theorem-proving-in-haskell.html",
    "href": "2015/12/02/theorem-proving-in-haskell.html",
    "title": "Theorem Proving in Haskell",
    "section": "",
    "text": "For some reason that is not completely clear to me either, I decided to go about and implement the sequent calculus technique for theorem proving this weekend. Because that alone would be a rather dull exercise, I decided to do it in a language I do not know very well. I first thought about trying to implement it in OCaml, but then decided to go with Haskell because there I at least know some basics.\nBefore going into the implementation, it is probably worth spending a few words on the sequent calculus. The basic construct of the sequent calculus is a sequent, which looks like this: \\(A, B, \\ldots \\vdash C\\). This is the assertion that, from the set of assumptions \\(A, B, \\ldots\\), we can prove \\(C\\). The sequent calculus consists of rules that allow to simplify the sequents in a way that the simplified sequents are valid iff the original one is. These rules can be repeatedly applied until only atoms exist in the sequent, at which point it is obvious whether it is valid or not (e.g. \\(A \\vdash A\\) is obviously correct, while \\(A \\vdash B\\) is not, with \\(A\\) and \\(B\\) being logical atoms).\nOne detail I have omitted so far is that I initially wanted to write a prover that works for intuitionistic instead of classical logic. The difference between the two logics is that, intuitionistic logic is stricter than classic logic in that it requires proofs to be constructive. This means that the proofs can be used to construct an object that satisfies the assertion. For instance, \\(P = NP \\lor P \\ne NP\\) is obviously true in classical logic, but intuitionistically one cannot prove this without knowing which of the two sides hold. If you are only here for the Haskell, it is probably not important that you completely understand this.\nMy initial attempt (I am very inexperienced in Haskell) was to define two functions to encode the proof with the first operation being on left or the right side of the sequent. It would then call the respective other function on the simplified sequents. At this point I only wanted to encode intuitionistic proofs, so I had the left hand side of the proof as a list of expressions, and the right hand side as a single expression. I had the constructor Nil, only to be used on the right side, to express that it is empty. proveLeft tried to apply a rule to the first expression in the list.\ndata Expr = Implication Expr Expr | Var String | Neg Expr | Nil deriving (Eq,Show)\ndata Sequent = Sequent [Expr] Expr deriving (Eq,Show)\n\nproveLeft :: Sequent -&gt; Bool\nproveRight :: Sequent -&gt; Bool\nI ran into all sorts of problems, especially with the negation operator on the left side, which can only be applied when the right hand side is empty. Atoms had to be explicitly skipped by putting them to the end of the list. Because of this it was hard to figure out when to terminate, or, as with intuitionistic logic one has the drop the right hand side if the proof is “stuck”, when to do that.\nI tried to make this work for a bit, but then realized if I ever got it to work, it will be extremely brittle and probably broken in one corner case or another. I then tried attacking the problem in smaller parts – the first one of which was transforming a sequent into simpler sequents. I also thought always using the first expression in the list as the one to operate on was not helpful, so I came up with the following functions (with rather poor names). I also decided to do classical logic first – in a sense, it is simpler because the left and the right hand side are both sets of expressions.\nexpandLeft :: Sequent -&gt; Expr -&gt; Maybe [Sequent]\nexpandRight :: Sequent -&gt; Expr -&gt; Maybe [Sequent]\nThe return value is Maybe [Sequent] so the function can return Nothing if the Expr cannot be operated on, or Just [Sequent] if it can be. For optimization purposes I decided to replace the [Expr] from before with a custom type containing three [Expr] – one for atoms (we need not bother trying to apply rules to them), one for negation (for classical logic, this is not needed, but later for intuitionistic we will want to skip those if there is something on the right hand side), and other composite expressions that we can always apply rules to. This is not strictly needed because of the Maybe [Sequent], but it felt a bit wasteful to call a function that only returns Nothing for every atom – would be interesting to do profiling on this.\ndata Expr = Implication Expr Expr | Var String | Neg Expr deriving (Eq,Show)\ndata ExprSet = ExprSet [Expr] [Expr] [Expr] deriving (Eq,Show)\ndata Sequent = Sequent ExprSet ExprSet deriving (Eq,Show)\nThe rest of the program involved a lot of using functions from Data.Maybe to put the pieces together.\nstepRight :: Sequent -&gt; Maybe [Sequent]\nstepRight s@(Sequent lhs rhs) = listToMaybe$mapMaybe (expandRight s) ((getComposite rhs) ++ (getNeg rhs))\nThis runs expandRight for all expressions on the right hand side, and returns the result first one of those that returns a Just [Sequent], or Nothing if all of them return Nothing. The right hand side is the same idea.\nThen, the same idea can be used to apply steps on the left side until there are none more to be taken, then on the right hand side. LK stands for the classical version of the sequent calculus there.\nstepLK :: Sequent -&gt; Maybe [Sequent]\nstepLK s = listToMaybe$catMaybes [stepLeft s, stepRight s]\nEven though I start with a single sequent, a step on a sequent can result in more than one. We need to apply stepLK to all sequents in a list until it returns Nothing for all of them. To do so a function steps is defined that returns either the derived list of Sequents after taking a step, or Nothing if no steps can be taken any more.\ngetFirst :: (Maybe a, a) -&gt; a\ngetFirst ((Just x), _) = x\ngetFirst (_, y) = y\n\nsteps :: [Sequent] -&gt; Maybe [Sequent]\nsteps xs = if all (isNothing . fst) nextIter then Nothing\n                                             else Just$concat$map getFirst nextIter \n        where nextIter = map (\\x -&gt; (stepLK x, [x])) xs\nThen, to bring this to the logical conclusion, this function is applied until it returns Nothing, and then it is checked whether all sequents in the call before that were axioms.\niterToNothing :: (a -&gt; Maybe a) -&gt; a -&gt; a\niterToNothing fn x = case fn x of Nothing -&gt; x\n                                  Just y  -&gt; iterToNothing fn y\n\nsolve :: Sequent -&gt; Bool\nsolve s = all isAxiom$ iterToNothing steps [s]\nTurning this into a solver for LJ (the intuitionistic version of the calculus) is fairly easy at this point. The rules operating on the negation on the left side have to be guarded by a condition that the right hand side be empty, and the stepsLK function has to be changed to drop the right hand side if there are no more steps for a sequent but it is not an axiom yet. In the implementation this leads to two functions being different between the LK and LJ cases, but I have yet to find a way to properly select which of the two to use at runtime: plumbing the functions through the computation by passing them as arguments does not seem the nicest way."
  },
  {
    "objectID": "2015/08/19/a-paper-a-week-arithmetical-hierarchy-and-complexity-of-computation.html",
    "href": "2015/08/19/a-paper-a-week-arithmetical-hierarchy-and-complexity-of-computation.html",
    "title": "A Paper a Week-ish #12: Arithmetical Hierarchy and Complexity of Computation",
    "section": "",
    "text": "The paper in question covers something that I have been meaning to read up for months: the arithmetical hierarchy, which was sadly not covered in the university courses I have attended. If you are into theoretical computer science, you might have come across things like \\(\\Sigma_2^0\\) or \\(\\Pi_1^0\\); I have, and while I knew that they had something to do with the arithmetical hierarchy (which I did not know what exactly it was, either), I never got around to read up on what exactly they mean. In particular, this paper examines where sets that make interesting statements about computability theory fit into the arithmetical hierarchy.\nFrom all I can tell, the paper does a very unfortunate mistake when defining the meaning of the two concepts, as it wrongly gives the same definition for both of them. The paper defines a set \\(A\\) of natural numbers being in both \\(\\Sigma_n^0\\) and \\(\\Pi_n^0\\) as \\(i \\in A \\Leftrightarrow \\forall k_1 \\exists k_2 \\ldots R(i, k_1, k_2, \\ldots)\\), where \\(R\\) is a computable predicate (otherwise every set would be in \\(\\Sigma_0^0\\) because every set is representable by a predicate, albeit not by a computable one). Of course, this does not make any sense, and Wikipedia offers a much more believable definition thereof: “Also, a \\(\\Sigma^0_n\\) formula is equivalent to a formula that begins with some existential quantifiers and alternates \\(n-1\\) times between series of existential and universal quantifiers; while a \\(\\Pi^0_n\\) formula is equivalent to a formula that begins with some universal quantifiers and alternates similarly.” In this context formula and set is more or less interchangeable, as a formula describes the set of values that satisfy it.\nAfter clearing up this slight confusion, it also becomes possible to understand the rest of the paper. It defines the concept of a \\(\\Sigma_n^0\\)-complete and \\(\\Pi_n^0\\)-complete set as being a set that all sets of the respective class can be reduced to and that is in that class. Let, for example, \\(B\\) be \\(\\Sigma_2^0\\)-complete, then for every set \\(A \\in \\Sigma_2^0\\) there is a computable function \\(f\\) such that \\(i \\in A \\Leftrightarrow f(i) \\in B\\).\nA few complete sets are given (without proof). I will pick out the example of \\(K = \\{i: 0^i \\in W_i \\}\\) where \\(W_i\\) is defined in this paper as the set of words for which the Turing machine of index \\(i\\) halts. For those unfamiliar with the theory of computation this may be odd, but as every Turing machine can be described by finite string over a finite alphabet, there is a way to enumerate the Turing machines, that is, assign an index to each of them.\nIt took some time wrapping my head around why this set is \\(\\Sigma_1^0\\), but essentially it boils down to the fact that it can be expressed by \\(i \\in K \\Leftrightarrow \\exists z\\; Halts_i(0^i, z)\\) where \\(z\\) expresses (the Gödel number of) a trace of a computation of \\(M_i\\) and \\(Halts_i(j, z)\\) is a predicate that is true iff \\(z\\) is a trace of a halting computation of \\(M_i\\) on input \\(j\\); without fancy terms the formula means: an \\(i\\) is in \\(K\\) if and only if there exists an execution trace of \\(M_i\\) that halts on the input \\(0^i\\). Well, less fancy terms, I guess.\nIf you know about computability theory you see I hand-waved a few things. For instance, I just assumed that \\(Halts\\) is a computable predicate, but it is not hard to imagine that checking whether a trace of a computation on a Turing machine on an input actually is a halting computation is indeed computable (you check whether the last state is a halting state and then check whether all steps of the trace are allowed in the Turing machine). It is also important to note that the restriction to computable predicates is needed for the definition to make any sense, this is because if non-computable predicates are allowed, every set \\(A\\) would trivially be expressible using zero quantifiers by a predicate \\(P(e) \\Leftrightarrow e \\in A\\).\nThe paper then shows that the set of (indices of) Turing machines that work in time \\(n + 1\\) relative to the size of their input is \\(\\Pi_1^0\\)-complete. They first show that the set is in \\(\\Pi_1^0\\) by showing a formula that represents it. It might be confusing to the reader that their formula is \\(\\forall n, x, z\\; (…)\\). At first look one might think “I thought \\(\\Pi_1^0\\) meant there is one \\(\\forall\\)”. This is a technicality that is not elaborated on in this paper: the universe is assumed to be integers, which are countable. This allows to encode n-tuples as a single integer in various way (e.g. using powers of prime numbers), which are then “decomposed” again by the predicates. The proof that it is complete is done by reducing a set already known to be complete to it (I will omit details to not completely blow the scope of this post).\nAfter this it is also proven that the set of Turing machines that work in polynomial time is \\(\\Sigma_2^0\\)-complete, the details of which I also refer to the paper. The main idea of the proof is again to prove that it is in \\(\\Sigma_2^0\\) by showing a formula that expresses it and then reduce an already known to be complete set to it.\nThere are more proofs about properties of Turing machines and their place in the arithmetic hierarchy which I will not elaborate on in the interest of brevity, if you are interested in this sort of stuff I recommend reading the whole paper. One of them sadly references another paper which is behind a paywall, so it might be hard to really grasp what is going on (one has to trust the statement asserted with a reference to that paper is true).\nAnyway, the second section of the paper shows corollaries that can be deduced from the previously proved statements about the arithmetical hierarchy. For example, that there is a Turing machine that works in time \\(n + 1\\) but cannot be proved to. This is simply because Turing machines that provably do so can be enumerated by definition – proofs are finite strings over finite alphabets, hence can be enumerated. The set of Turing machines that have the property has previously been proven to be \\(\\Pi_1^0\\)-complete, which cannot be enumerated. This is, intuitively, because infinitely many elements have to be checked before giving an answer to the \\(\\forall\\).\nI will not enumerate the rest of the proofs and corollaries, so take a look at the paper if this incited interest.\nLink to the paper: Arithmetical Hierarchy and Complexity of Computation."
  },
  {
    "objectID": "2015/07/19/a-paper-a-week-brewers-conjecture.html",
    "href": "2015/07/19/a-paper-a-week-brewers-conjecture.html",
    "title": "A Paper a Week-ish #11: Brewer’s Conjecture and the Feasibility of Consistent, Available, Partition-Tolerant Web Services",
    "section": "",
    "text": "This paper was more discovered than chosen. I decided to write this week-ish’s post sitting in a train without Internet connection, so I scanned my Download folder for the random papers I have accumulated over the months. I discovered this paper, which seemed interesting and brief enough so I could finish reading it on the train.\nIt is, as is fairly obvious, about Brewer’s conjecture. To be honest, I have never actually heard of this before (and I might just have mistaken it for Brouwer, the logician, back when I downloaded this paper, but I guess I will never know). It seems to state the fact that it is impossible to achieve consistency, availability and partition-tolerance at the same time.\nThe first section of this paper talks about these properties in a fully asynchronous system – that is a system where every agent must function only based on the messages it has received. Most statements made are fairly self-evident. The main theorem of this part states that if the network is partitioned into two or more parts, a write made in one of them will not be reflected in reads in others’. This isn’t a very deep observation, in my opinion, as the side condition that every request be served eventually is obviously inconsistent with partition-resistant consistency. It cannot even be both available and consistent even in the case where all messages are delivered, because a node has no way of delaying the response to the client until a point where it knows it would have received all messages preceding the request by the client.\nThe second section is a bit more interesting. It analyzes these properties in a what it calls partially synchronous network. These are networks in which every node has a clock running at the same rate and there are bounds on network latency – mind the difference to systems having the absolute result of clocks synchronized to some degree, like TrueTime used in Google’s Spanner. I am sure a physicist would tell me that having two clocks run at exactly the same rate is technically impossible, but we can probably get close enough for practical purposes (and this is a model anyway).\nAnyway, in these networks the main theorem of the previous section obviously still holds. Even such a probably-physically-impossible system is incapable of magically bridging arbitrarily long network partitions in finite time. However, the variant where all messages are delivered does not hold, because assuming a central node that manages the system, a node can always serve a request by waiting for the appropriate amount of time before returning because there are bounds on the network latency (of course in practice there is not much point in having a distributed data-base managed completely by a central node).\nA weaker form of consistency is then proposed, which is based on consistency while all messages are delivered, and a bound on re-convergence time after a interval in which not all messages have been delivered. This notion is formalized and then an algorithm that achieves this is shown.\nFind the paper here: Brewers Conjecture paper."
  },
  {
    "objectID": "2015/02/27/a-paper-a-week-openjdk-sort-broken.html",
    "href": "2015/02/27/a-paper-a-week-openjdk-sort-broken.html",
    "title": "A Paper a Week #1: OpenJDK’s java.util.Collection.sort() is broken: The good, the bad and the worst case",
    "section": "",
    "text": "I’m planning to read at least one paper a week about computer science or logic related topics and comment a bit on this blog. This is the first post in the series (let’s see how long I am able to keep this up).\nThe paper thus discussed has caught my attention because while I am really interested in and like logic and theoretical computer science, I am always ambiguous about the practicability of the field. This makes me happy about a paper that shows real world problems being solved using tools (theorem proving) that would not be possible without extensive research in both fields.\nSo, what is the paper actually about? It documents the findings of a group of researchers that have uncovered a bug in Java’s TimSort implementation (and, actually, the algorithm TimSort itself) that results in Collection.sort throwing an index out of bound exception, which is the last thing one would expect it to do. As the authors point out in the introducing paragraphs, this is particularly interesting because the bug occurs in OpenJDK, a well-tested and well-studied piece of code. This shows that the bug occured despite unit-testing practices.\nThe authors then go on formally verifying the properties that make sure the exception can never be raised in their fixed version. While one could argue that actually, the authors did not present a practical application of theorem proving, as the counter-example is obviously very carefully crafted to trigger the bug (and apparently wasn’t triggered as no one was aware of the bug), one has to consider two things. Firstly, the bug possibly offers an exploit for malicious attackers to remotely crash an application that accepts user data, and, secondly by Murphy’s law, by throwing enough data at Collection.sort, eventually one will hit a bad case.\nWhile the paper certainly shows that it is possible to use formal methods to an advantage in the software industry (a practice that is common in the hardware industry nowadays, following the extremely expensive Intel FPU bug), this would, unless theorem proving methods improve, require an almost impossible amount of change in the industry. One must consider that it took experts in the field guiding the steps of the interactive verification where, quote, approximately 5-10% required ingenuity, such as introducing crucial lemmas and finding suitable instantiations of quantifiers (“Q-inst”). I do not see this happening in a lot of contexts.\nSo, without further ado, here is a link to the paper for those interested: PDF."
  },
  {
    "objectID": "2015/05/07/a-paper-a-week-complexity-of-theorem-proving.html",
    "href": "2015/05/07/a-paper-a-week-complexity-of-theorem-proving.html",
    "title": "A Paper a Week-ish #7: The Complexity of Theorem-Proving Procedures",
    "section": "",
    "text": "First of all, sorry for the delay! I have moved this weekend (with a few complications) and am currently without Internet at home, and Three are actively trying to prevent me tethering from my phone (and I don’t want to violate the contract).\nThis is actually, for computer science, a fairly old paper, written by no less than Stephen Cook, about whom you might have heard, for instance regarding the Cook(–Levin) theorem, which in fact is closely related to the content of this paper. Essentially, Cook’s theorem states that the propositional case of satisfiability is NP-complete.\nFor those not familar with complexity theory (very roughly the theory for determining the relative hardness in terms of computational effort of different problems), NP-complete problems are those that are mostly believed to be hard to solve (that is, there exists no polynomial algorithm), but no proof exists for that statement. Solving – as in coming up with an algorithm of polynomial running time, one would immediately solve the whole class. This is due to the observation that is also expressed in the paper that there exist polynomial reductions between the problems (a reduction is a transformation of an instance of one problem into an instance of another, whereby one can solve the initial problem).\nThe first half of the paper talks about the complexity of theorem proving in the propositional case, which is essentially the same as satisfiability in classical logic (something is a tautology iff its negation is unsatisfiable)\nThe second half of the paper concerns itself with the complexity of theorem-proving first-order (i.e. predicate) logic and presents lesser known ideas. It is generally known that theorem proving is undedicidable, that is, there is no algorithm that can proof all valid theorems of first-order logic. Thus, the definition of complexity is a bit more evolved than in the propositional case. How can one analyze an algorithm that can possibly go on forever without halting? Cook essentially proposed setting an order on the Herbrand instances (essentially instantiating every function and predicate in every possible way) to analyze the complexity relative to the highest index of the instances used in a proof (this is a bit of a wandwavey explanation). He presents two bounds on this measure.\nFind the paper here (actually, this is a re-typeset version, because the original one was written on a typewriter and this is, in my opinion, nicer to read)."
  },
  {
    "objectID": "2015/03/15/a-paper-a-week-doppler-effect.html",
    "href": "2015/03/15/a-paper-a-week-doppler-effect.html",
    "title": "A Paper a Week #3: SoundWave: Using the Doppler Effect to Sense Gestures",
    "section": "",
    "text": "I almost feel like I cheated by picking such a short paper – it has been a busy week, though. As the paper is only 4 pages long, this post won’t be particularly long, either.\nThe paper presents a way of sensing gestures with two pieces of hardware found in almost every laptop that has recently been produced: a microphone and a loadspeaker. The loudspeaker generates a tone in the inaudible range that can be produced by off-the shelve loudspeakers: around 18 – 22 kHz. An initial calibration step determines the exact frequency, as some may be impossible to produce on a given setup. This is then recorded by the microphone, the signal is Fourier-transformed using the FFT. After some data sanitising based on e.g. maximum observable gesture speed (the wording is quite funny: “Informal tests with multiple people indicated that the fastest speed at which they could move their hands in front of a laptop was about 3.9 m/sec”), movement is detected by a significant frequency shift, and velocity, direction and proximity to the device are extracted. I think this is a valuable presentation of the usefulness of the FFT in real-world software applications.\nUsing the Doppler-effect to detect movement certainly is not a new idea, but the paper presented that it is viable to use hardware generally available rather than specifically built higher-frequency equipment.\nThe paper can be obtained here: PDF."
  },
  {
    "objectID": "2015/03/08/a-paper-a-week-dpllt.html",
    "href": "2015/03/08/a-paper-a-week-dpllt.html",
    "title": "A Paper a Week #2: DPLL(T): Fast Decision Procedures",
    "section": "",
    "text": "If you are not interested in reading about I paper I meant to read and cover here but eventually gave up doing so, skip the next two paragraphs.\nBecause I was interested in reading about Z3, the SMT (satisfiability modulo theory) solver developed by Microsoft Research, I originally tried to read “Tractibility and Modern Satisfiability Modulo Theories Solvers” because I – wrongly, as it would sadly quickly turn out – assumed to would explore some deep connection between tractibility and SMT solvers I have not previously heard of. I started reading and it would offer a fairly good reasonably high level introduction into SMT and DPLL and some of its applications.\nThen it turns to listing examples for tractible and intractible theories, elaborating them to various degrees; I am not sure how this adds to the rest of the paper, but I stopped reading the paper about 2 thirds in, so I might have missed something. Especially the section about the theory of arrays was impossible to understand, as from all I can tell, they define \\(C_1 = a \\lor \\neg b\\), only to later use it as \\(read(C_1, \\ldots)\\), which looks like applying a function to a formula, which is odd. I am really not sure about the notation there; maybe I don’t understand “We will use the array M to encode a propositional model, and for each clause \\(C_i\\) use a corresponding array to encode selecting a literal that evaluates to true in the model” correctly, because I am not sure what it is supposed to mean.\nThe paper referenced “DPLL(T): Fast Decision Procedures”, which introduces the general algorithm used by Z3, and because I have been running out of time trying to understand the Microsoft Research paper, I decided to go for that one for this post instead. It introduces the DPLL(T) method, which is basically propositional DPLL extended by some theory. I should probably elaborate a bit more on that at this point. DPLL is (very roughly) based on the observation that, imagine you have \\(a \\lor b\\) and you know that \\(\\neg a\\), then you can conclude \\(b\\). Now, if you have a formula in CNF (conjunctive normal form) and want to check its satisfiability, you can branch on any atom arbitrarily (i.e., you just assume that some atom \\(a\\) is true, and see if that leads to a satisfying interpretation, if it does not, assume it is false and do the same), then propagate the information as in the example before (this process is called unit propation), hopefully get the value for more atoms, repeat this process on them until you can no longer propagate and then branch on another unset atom.\nCountless improvements and heuristics have been thought up, but will not be covered in this post as to keep this post at a sane length. The paper is not about those, anyway, but rather a new way to adapt the system to be able to handle formulas over a theory. Basically, a theory is a set of symbols with related axioms that force them to behave the way they are supposed to. The paper considers the theory EUF, which stands for equality and uninterpreted functions, which essentially makes sure that equality signs are interpreted with the usual meaning (also in the presence of functions, such that \\(f(a) \\ne f(b)\\) implies \\(a \\ne b\\)). Techniques predating the paper achived that by adding applicable axiom instantiations (e.g. add \\(a = a\\), \\(b = b\\), … to make sure equality is reflexive) either at once or incrementally until a model consistent with the theory was found; hybrid approaches were also proposed.\nThe technique introduced in the paper, on the other hand, solves the problem by defining a modular interface that interfaces solvers for a theory to the general DPLL algorithm. This can be thought of this way: to the DPLL part, all atoms are purely syntactic as in the propositional case. For instance \\(a = b \\land b = c \\land a \\ne c\\) is of the form \\(A \\land B \\land \\neg C\\) and is satisfiable by \\(v(a = b) = 1\\), \\(v(b = c) = 1\\), \\(v(a = c) = 0\\). Of course, this model is inconsistent to the theory of equality, which the solver for the theory will detect; it will also take into account the semantics of the theory for unit propagation.\nSo, I guess this post is long enough as it is, so without further ado, a link to the paper: PDF."
  },
  {
    "objectID": "2015/03/31/a-paper-a-week-temporal-audio-synchronization.html",
    "href": "2015/03/31/a-paper-a-week-temporal-audio-synchronization.html",
    "title": "A Paper a Week #5: Temporal Synchronization of multiple Audio Signals",
    "section": "",
    "text": "This week’s paper was published by Google and concerns itself with finding methods of combining overlapping audio streams that were recorded from different locations, which is often the case, for instance in concert recordings. It suggests use-cases such as 3D scene reconstruction, which could be done by combining videos that were first synchronized using their audio stream.\nThe paper starts by explaining that existing techniques for matching audio streams basically work by finding a locally good match and then trying to expand that match further. This local approach is contrasted by a global one presented here.\nIn fact, the paper shows two different implementations of what is roughly the same general idea. The first method proposed is, at least to me, very appealing in its simplicity and elegance. It goes as follows: first, an offset is calculated between every pair of input signals. This can be done in several ways, one of which is maximizing the cross-covariance (think of it as a measure of correlation) of “spectral flatness”, which is described as being “the variation of tonality over time. It is defined by the ratio of the geometric mean and arithmetic mean of the frequency domain coefficients”.\nThen, a complete graph containing each signal as nodes is constructed, and a measure of “how good” the matching between any two signals is (according to some metric) is used as the edge weight between the two. On this, MST (minimum spanning tree) is applied to find a global solution.\nThe second method (very roughly) proposes a probabilistic distribution for the offset between two signals, based on the offsets calculated as above. Then, a method called Belief Propagation (which I hope I can cover in greater detail at a later point in this series) is used to calculate the marginals of that distribution. Because this not necessarily converges to a global solution, different base hypothesis have to be tried. I find the notation in this part of the paper a bit confusing and I had to make a few assumptions about what they are trying to say when reading it (so I might be summarizing that incorrectly!).\nThen follows an experimental evaluations that shows both methods perform well.\nWithout further ado, here is the paper: PDF."
  },
  {
    "objectID": "2015/03/23/a-paper-a-week-model-checking-distributed.html",
    "href": "2015/03/23/a-paper-a-week-model-checking-distributed.html",
    "title": "A Paper a Week #4: Parameterized Model Checking of Fault-tolerant Distributed Algorithms by Abstraction",
    "section": "",
    "text": "First of all, sorry for this post being delayed. But with moving to London and starting a new job, I guess it is not the worst week to give myself a break. Anyway, while there is a post, for this paper, as it is particularly involved (though very concentrated on 8 pages), I will start with a more high-level post and follow up in a later post with more technical aspects.\nSo I guess for the unfamiliar reader, the title needs some further elaboration. Model checking refers to a technique for automatically verifying properties of computer programs. This is very roughly done by restriciting the types of variables occuring in the program, checking for counter-examples in that simplified program, check whether it is possible for them to appear in the real program, and, if not, refine the simplification.\nThis paper introduces a method to make this also applicable for a kind of distributed algorithm. The paper concerns itself with algorithms which provide fault-tolerance up to a degree, i.e. they guarantee they are behaving correctly even if up to a predefined fraction of the processes contributing to it are misbehaving.\nThe problem with that is, though, that generally the number of processes and thus of mishaving processes can be arbitrary. Verifying the system for fixed parameters \\(n\\) of total processes and a maximum of \\(t\\) faulty ones is unfeasible for large values thereof due to state space explosion. This is where the parametrized model checking comes in.\nAll processes are collapsed into a single large system where a state transition represents one of the processes taking a single step. This is then used to verify LTL formulas against the system, i.e. it is checked whether for all parameters compatible with the maximum amount of failing processes, the property holds on all possible paths.\nThe details on how this is done will be covered in a follow up post.\nFor those who want to skip ahead and find out themselves: PDF."
  },
  {
    "objectID": "2015/04/12/a-paper-a-week-differential-privacy.html",
    "href": "2015/04/12/a-paper-a-week-differential-privacy.html",
    "title": "A Paper a Week #6: Differential Privacy",
    "section": "",
    "text": "First, I will, for the coming few weeks, reduce the frequency of these posts to one a fortnight – for one, fortnight is an amazing word, and also I am rather busy with moving (and working), so I will resume to the usual frequency after I have settled a bit.\nWith this out of the way, now on to talking about papers! This week’s paper was published by Cynthia Dwork (the co-inventor of differential privacy) of MS Research and is about differential privacy.\nWhat is that, you ask? Differential privacy is a measure introduced to reason about privacy leaks (affecting individuals) caused by statistical databases. Differential privacy aims to make certain guarantees of protecting individuals’ privacy even in the presence of auxilliary information to an attacker. This concept is introduced in the paper by an example (which is an excellent thing to do in a paper if you want people to actually understand what you are talking about): imagine an attacker knows a person is 2 inches smaller than the average citizen of Urugay, then a query giving the average height of a citizen of Urugay, in combination with this auxilliary knowledge, reveals (more or less) personal information about the person.\nThis also nicely illustrates that it is impossible to design a database that, combined with arbitrary auxilliary information, never reveals any personal information. Thus, the aim of this paper is more modest: to design databases where participating in the database does not significantly increase the chance that personal information is revealed.\nEssentially, this is achieved by fuzzing the output enough so that it does not differ for any two database configurations only differing in one entry. I will skip the details and refer to the paper, which presents them in a quite understandable fashion (assuming you are concentrating while reading it).\nAnd yes, the paper stops as randomly and abrupt as this blog post.\nFind the paper here: PDF."
  },
  {
    "objectID": "2015/05/24/a-paper-a-week-reflections-on-trusting-trust.html",
    "href": "2015/05/24/a-paper-a-week-reflections-on-trusting-trust.html",
    "title": "A Paper a Week-ish #8: Reflections on Trusting Trust",
    "section": "",
    "text": "So, I feel like cheating a bit again as this paper is only three pages long, but I guess it is an ACM Turing Award lecture paper, so it deserves a spot here, at least for historical significance. Also, the paper is written by the highly influential (well – which goes without saying considering the Turing Award) Ken Thompson.\nIn the introduction he argues that while people may expect him to talk about UNIX, because this is what he is most famous for, he will not because he feels that he is getting undeserved credit for the work of a multitude of people where for his contributions luck and fortuate timing played as much part as skill. He also promises to show the “cutest program” he has written, but I am not sure how that is followed up on in the paper.\nIt talks about self-printing programs before concerning itself with self-hosting compilers, that is, compilers for a language that are written in that same language, for instance a C compiler written in C. This reminded me a bit of assembling an IKEA desk yesterday that required a table for assembly, so you have the same chicken-and-egg problem. The solution, of course, is relatively straightforward and consists of writing an initial version of the compiler in another language (or getting an table that is not this IKEA desk – or indeed in this case not following the instructions, but that is irrelevant to a discussion about computers), and then using existing binaries of the compiler to compile subsequent versions.\nThe example he shows is a bit more interesting. He considers a high-level idealized section of C compiler that treats string escape sequences, basically as a sequence of ifs like\nif (c == 'n') {\n    return '\\n';\n}\nThis is interesting for itself, as once again this is a self-reference that expects this to be compiled by a thing that already understands what '\\n' means. Now consider the case when you want to add a new escape sequence that is not available in your current compiler. Obviously the above will fail, as your compiler will complain that it does not know what to do with '\\n'. However, what you can do is add the following the the compiler, creating a version that understands the meaning of '\\n'. ’\nif (c == 'n') {\n    return 10; // 10 is the ASCII code for \\n.\n}\nYou can now, in any subsequent version of the compiler that is compiled by this (or a newer one), refer to '\\n' instead of 10. He claims that ‘It is as close to a “learning” program as I have seen’, which I would take with a grain of salt (especially honouring the fact that this paper was written in the 70s) considering recent advances in machine learning, whereas this is more of a matter of where a particular piece of data was introduced (the compiler, the compiler’s compiler, …) than anything that has genuinely to do with learning – the fact that it should return 10 for 'c == 'n' is in the compiler’s binary, either way.\nThen he, as is quite obvious, applies these ideas to information security. How, in face of this ridiculous matryoshka doll situation, can one be sure what a program does – in particular, that it does not do anything malicious? The only way would be to go up the chain of compilers until the root, which does not appear tractible. He suggests the alternative is trusting the people that wrote the program, thus the title of the paper (as the trust, indeed, has to be recursive – you implicitely trust the compiler that was used to compile your compiler).\nHe then ends with a few political comments about persecution of “hackers” and the two-facedness of considering them wiz kids and criminals at the same time, which sadly still seem current now over 40 years later.\nFind the paper here."
  },
  {
    "objectID": "2015/10/09/a-paper-a-week-propositions-as-types.html",
    "href": "2015/10/09/a-paper-a-week-propositions-as-types.html",
    "title": "A Paper a Week-ish #13: Propositions as Types",
    "section": "",
    "text": "Even though week-ish is an intentionally vague concept, I should apologize for this week-ish to take particularly long. The last month has been very busy and exhausting for me, so even though I read a paper, I never got to finish this post until now.\nThis paper presents no original insights, but is a very readable introduction into the Curry-Howard isomorphism and concepts related to it. It does not go into the complex technicalities but manages to convey a general intuition and interesting historical context. The appendix of the online edition (the edition I read) also includes an insightful correspondence between the author (Philip Wadler) and Howard about the various people involved in its history.\nIt introduces the concept in three simple statements.\n\npropositions as types\nproofs as programs\nsimplification of proofs as evaluation of programs\n\nWhat does this mean? This means that there is a bijection between proofs in a formal proof system and programs in a formal system of computation, where propositions in the proof correspond to types in the program.\nThis is a very general statement, and the paper in question shows it for one particular pair of formal proof system and formal system of computation, in particular natural deduction for intuitionistic logic and singly typed lambda calculus. It then notes that it indeed holds in various other cases, the list of which I will not enumerate here.\nIf one thinks about the Brouwer–Heyting–Kolmogorov interpretation of intuitionistic logic, which is also introduced in the paper, this correspondence becomes intuitively plausible. The BHK interpretation defines the logical connectives by the existence of procedures to turn proofs of a proposition into another proof.\nFor example, a proposition \\(A \\rightarrow B\\) holds if there is a procedure that turns proofs of \\(A\\) into proofs of \\(B\\). So if we look at a bijection between propositions and types, this means that there is a procedure that turns an object of type \\(A\\) into one of type \\(B\\). Also other propositions are expressed as objects in the BHK interpretation. For example, \\(A \\land B\\) holds if there is a tuple \\(\\langle a, b \\rangle\\), where \\(a\\) is a proof for \\(A\\) and \\(b\\) is a proof of \\(B\\), so it can be seen as a tuple. \\(A \\lor B\\) can similarly be seen as the option type for an object that is either of type \\(A\\) or \\(B\\).\nThe paper does a much better job of explaining this all in a bit more detail than I ever could in the course of a blogpost here, so I will not attempt to, but I hope this will bring across the very basic idea behind the concepts.\nI also recommend browsing the titles in the reference section if you are interested in the content provided by this paper, I have highlighted [23], [33], [40], [42], [54], [56] and [59] (which does not mean the other ones are worse, just that they did not immediately catch my eye) for further study (probably I will only get to the papers, not the textbooks).\nFind the paper here: Propositions as Types."
  },
  {
    "objectID": "2015/07/06/a-paper-a-week-understandable-consensus.html",
    "href": "2015/07/06/a-paper-a-week-understandable-consensus.html",
    "title": "A Paper a Week-ish #10: In Search of an Understandable Consensus Algorithm",
    "section": "",
    "text": "I have been meaning to read this paper for quite some time, but for some reason, even though its entire point is proposing an understandable consensus algorithm, Paxos’ (the canonical consensus algorithm previous to this paper) reputation for being impossible to understand kept me from reading this paper whenever I was short on time – that is to say always. Probably I was assuming that an “understandable” replacement for an impossible-to-understand is still very hard to grasp and I would not have time to properly digest it. Thankfully, I could not have been wronger.\nRaft, the algorithm proposed in the paper, is a solution to the replicated state machine problem. The basic premise thereof is a cluster of computers running independently but working together in what appears to be a single consistent state machine. Without auxiliary conditions, this is a quite simple goal to achieve: one could simply choose one of these computers to actually do the work and leave the rest idle. Of course this is not the point of the exercise, so we want the state machine to be available as long as the majority of these computers is still up, while still giving only correct result. The paper refers to these properties as availability and safety, respectively.\nThe problem is simplified vastly by requiring that one of these computers is master at any given time, and clients only talking to this master. This is achieved by a fairly simple protocol where, upon detecting the current master has failed, a server initiates an election by talking to all other servers, asking them to vote for it. Other servers will vote only for other servers that are more up-to-date. If more than half of the servers vote for it, it becomes the new master. The protocol also shows that simplicity was the main design goal of Raft: to prevent servers from competing for the vote, which may lead to elections that need to be repeated for lack of winner, the server simply waits for a random time before calling the election, which makes it unlikely that two or more servers do so at the same time.\nThis server is then responsible for dealing with users requests. If a user wants to update the state, this change is written into the so-called log of the master and replicated onto the other servers called followers. Under normal operation, it will be considered to be committed and persistently in the system when it is written to more than half of the cluster (some caveats apply). Any data that was committed will be available as long as more than half the servers are up at any given point. The master always makes sure that when writing an entry on another server, the entry immediately before the one written matches the one in its logs. By induction, this means the follower’s log is complete up to the newly written entry.\nIf any of the servers except the master fails, nothing interesting happens. The master will just no longer be able to talk to it, it will no longer write data, but updates will be resent in case it comes back. Simple failover on master failure is equally straightforward. The master dies, the remaining servers elect a new master that has the same state as the previous master (because, remember that the updates are replicated to more than half of the servers). This new master takes the place of the old one, and nothing particularly interesting happens either.\nOf course, no one would care if the paper was simply about trivial failover. The interesting case arises, of course, due to more complicated series of master failures. But because of the simplicity of the algorithm this is, in fact, not much harder to understand than the trivial cases above. In this blog-post I have hand-waved “more up to date“ and “committed” a bit earlier, these two concepts are important to understand the general case. All log entries are identified by two integers: the term (the term increases every time the mastership changes), and a sequence number within that term. A server can only win the mastership election if either its term is higher than more than half of the others, or it is in the same term but has a longer (or equally long) log in it. This makes sure that the elected master always has a complete log of committed entries. An entry is committed only after the current master has written a log-entry in its term to ensure that more than half of the followers catch up with the master.\nFind the paper here: Raft Algorithm Paper."
  },
  {
    "objectID": "2015/06/16/a-paper-a-week-algorithms-for-maintaining-order-in-a-list.html",
    "href": "2015/06/16/a-paper-a-week-algorithms-for-maintaining-order-in-a-list.html",
    "title": "A Paper a Week-ish #9: Two Simplified Algorithms for Maintaining Order in a List",
    "section": "",
    "text": "Looking around for papers to cover in this series, I remember a talk about folding algorithms one Erik Demaine gave at my former university as the annual Vienna Gödel Lecture, where once a year major contributors to the field of computer science (other guest lecturers were Don Knuth and Peter Norvig) are invited to give a talk about their work. I skimmed through his very impressively long list of papers and this one caught my eye. If you have a bit of time on your hands and are interested in algorithms (he seems to specialise a lot in folding and puzzles), I recommend you do the same.\nAnyway, I picked this paper more or less at random from the seemingly endless list, and it, as the title says, proposes two algorithms for maintaining order in a list. As is usual, the first part of the paper is dedicated to giving a general introduction into the problem, its’ history and the solutions that have previously been proposed to solve it. Then, they justify the first of their two algorithms which, even though it does not offer theoretical improvements (i.e. lower complexity bounds – which is impossible considering that the best solution has \\(O(1)\\) worst-time on all operations), has two characteristics that make it useful: their analysis is easier to follow, and their practical experiments suggest that its’ real-world performance is superior. In fact the theoretic performance is arguably worse: the proposed data structure has \\(O(1)\\) amortised time complexity, while the best known one has \\(O(1)\\) worst-case. Anyway, as is sometimes the case, there are cases where the theoretically worse data-structure has better real-world performance; as computers always work with finite data, asymptotic performance is sometimes an oversimplification.\nThe first algorithm is a solution to the Order-Maintenance Problem, which has “the objective … to maintain a total order subject to insertions, deleteions and precendence queries”. It does that by assigning tags to elements such that the order of the tags corresponds to the order of the elements. Then, the analysis proceeds on the virtual trie that can be obtained using the binary expansion of the tags. Roughly the algorithm makes sure that the density (that is, the number of “occupied” tags per available tag) does not exceed a threshold for every node in the implicit trie. The analysis is a bit handwavey at times, for instance it states that “Thus, we pick a u [comment: maximum tag] that works for any value from n/2 to 2n. If at any point we have too few or too many list elements, we rebuild the data structure for the new value of n. This rebuilding introduces only a constant amortized overhead.” I can see this statement being true in analogy to how amortized analysis of dynamic arrays works (that is, always doubling the size of the array when it is full results in amortized \\(O(1)\\)), but it is not immediately obvious that the statement is true, because there could be subtle differences.\nAn important detail of the complexity analysis is that it does not do any assumption about which tag is chosen for a newly inserted element. If there are multiple tags free between the elements \\(e\\) and \\(f\\) adjacent to the newly inserted one, one would assume that it is best to chose the new tag such that it is as far from both as possible. This not being represented in the theoretical analysis could be a sign that the bound is simply not tight enough (i.e., that one could achieve a better bound by considering this), but their experimental evaluation suggests otherwise. It seems to make no difference where the new element is inserted, in their own words “Consecutive inserts trigger relabels after every insert, but the number of elements relabled is small. … average inserts have fewer triggers, and the number of elements relabled is larger.”\nThe second algorithm is a solution to the File-Maintenance Problem, which is similar to the Order-Maintenance Problem but also requires that it is possible to traverse the elements in order. The solution keeps the elements in order in an array. Again, the algorithm works on an implicit tree where \\(a \\log n\\) elements of the array are bundled into one leaf node, for some \\(a\\). The algorithm then makes sure that two invariants are preserved, such that there is always space to insert a new item. Firstly, adjacent intervals (on every level of the implicit tree) must only differ by a certain amount. Secondly, every interval (on every level of the implicit tree) must have sufficient space to accommodate new items. The algorithms that make sure this is the case are too sophisticated to be covered in this summary, so consult the full paper for these details.\nFind the paper here."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Strings over Σ",
    "section": "",
    "text": "Linux: The good, the bad and the ugly\n\n\n\n\n\n\n\n\nApr 19, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nAutomatic Org-Mode\n\n\n\n\n\n\n\n\nApr 16, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysing regexp with Golang\n\n\n\nprogramming\n\n\n\n\n\n\n\n\n\nMar 1, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nMore Turing Completeness, or: Somebody is wrong on the Internet\n\n\n\ncomputer-science\n\n\n\n\n\n\n\n\n\nNov 23, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nFun with Operator Overloading\n\n\n\npython\n\n\n\n\n\n\n\n\n\nOct 14, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nCompleteness and decidability\n\n\n\nlogic\n\n\n\n\n\n\n\n\n\nSep 25, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nA Paper a Whatever #15: Keystroke Recognition Using WiFi Signals\n\n\n\napaw\n\n\n\n\n\n\n\n\n\nSep 3, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nTuring Completeness\n\n\n\ntheoretical-cs\n\n\n\n\n\n\n\n\n\nAug 27, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nA Paper a Week-ish #14: GraphLab: A New Framework for Parallel Machine Learning\n\n\n\napaw\n\n\n\n\n\n\n\n\n\nApr 17, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nUsing theorem proving to cheat in logic puzzles\n\n\n\nlogic\n\n\n\n\n\n\n\n\n\nApr 3, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nTheorem Proving in Haskell\n\n\n\nprogramming\n\n\n\n\n\n\n\n\n\nDec 2, 2015\n\n\n\n\n\n\n\n\n\n\n\n\nA Paper a Week-ish #13: Propositions as Types\n\n\n\napaw\n\n\n\n\n\n\n\n\n\nOct 9, 2015\n\n\n\n\n\n\n\n\n\n\n\n\nA Paper a Week-ish #12: Arithmetical Hierarchy and Complexity of Computation\n\n\n\napaw\n\n\n\n\n\n\n\n\n\nAug 19, 2015\n\n\n\n\n\n\n\n\n\n\n\n\nA Paper a Week-ish #11: Brewer’s Conjecture and the Feasibility of Consistent, Available, Partition-Tolerant Web Services\n\n\n\napaw\n\n\n\n\n\n\n\n\n\nJul 19, 2015\n\n\n\n\n\n\n\n\n\n\n\n\nA Paper a Week-ish #10: In Search of an Understandable Consensus Algorithm\n\n\n\napaw\n\n\n\n\n\n\n\n\n\nJul 6, 2015\n\n\n\n\n\n\n\n\n\n\n\n\nA Paper a Week-ish #9: Two Simplified Algorithms for Maintaining Order in a List\n\n\n\napaw\n\n\n\n\n\n\n\n\n\nJun 16, 2015\n\n\n\n\n\n\n\n\n\n\n\n\nA Paper a Week-ish #8: Reflections on Trusting Trust\n\n\n\napaw\n\n\n\n\n\n\n\n\n\nMay 24, 2015\n\n\n\n\n\n\n\n\n\n\n\n\nA Paper a Week-ish #7: The Complexity of Theorem-Proving Procedures\n\n\n\napaw\n\n\n\n\n\n\n\n\n\nMay 7, 2015\n\n\n\n\n\n\n\n\n\n\n\n\nA Paper a Week #6: Differential Privacy\n\n\n\napaw\n\n\n\n\n\n\n\n\n\nApr 12, 2015\n\n\n\n\n\n\n\n\n\n\n\n\nA Paper a Week #5: Temporal Synchronization of multiple Audio Signals\n\n\n\napaw\n\n\n\n\n\n\n\n\n\nMar 31, 2015\n\n\n\n\n\n\n\n\n\n\n\n\nA Paper a Week #4: Parameterized Model Checking of Fault-tolerant Distributed Algorithms by Abstraction\n\n\n\napaw\n\n\n\n\n\n\n\n\n\nMar 23, 2015\n\n\n\n\n\n\n\n\n\n\n\n\nA Paper a Week #3: SoundWave: Using the Doppler Effect to Sense Gestures\n\n\n\napaw\n\n\n\n\n\n\n\n\n\nMar 15, 2015\n\n\n\n\n\n\n\n\n\n\n\n\nA Paper a Week #2: DPLL(T): Fast Decision Procedures\n\n\n\napaw\n\n\n\n\n\n\n\n\n\nMar 8, 2015\n\n\n\n\n\n\n\n\n\n\n\n\nA Paper a Week #1: OpenJDK’s java.util.Collection.sort() is broken: The good, the bad and the worst case\n\n\n\napaw\n\n\n\n\n\n\n\n\n\nFeb 27, 2015\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "2020/04/16/organizing-notetaking.html",
    "href": "2020/04/16/organizing-notetaking.html",
    "title": "Automatic Org-Mode",
    "section": "",
    "text": "With the unprecedented en vogue1 right now, I have taken the unprecedented step of trying out Emacs after having been a loyal Vim user for years. This is not going to be another charge in the endless Editor War2. Rather more uncontroversially, this is going to focus on note-taking and how technology can (and cannot) help.\nIf how this relates to Emacs is a mystery, just trust me that this will make sense. But first to more elementary things."
  },
  {
    "objectID": "2020/04/16/organizing-notetaking.html#footnotes",
    "href": "2020/04/16/organizing-notetaking.html#footnotes",
    "title": "Automatic Org-Mode",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf anyone happens to read this in the future: this was written at the time of the global COVID-19 lockdowns.↩︎\nVim keybindings are better.↩︎\nIf you have never tried dotted paper, do it. It’s like squared paper with less visual clutter. I like the Rhodia A4 notebook.↩︎\nWhich I, inconveniently, have no access to right now.↩︎\nJinHao make insanely affordable ones. I promise I do not get paid for these Amazon links.↩︎\nThere will need to be some more breakthroughs in machine learning before I can subject my handwriting to OCR software, and until then I’ll have to continue to search manually.↩︎\nUniversity is easier. Things can be neatly arranged by subjects.↩︎\nActually, I just poll the active window every 100 ms to see if it changed.↩︎"
  },
  {
    "objectID": "2018/03/01/golang-regexp.html",
    "href": "2018/03/01/golang-regexp.html",
    "title": "Analysing regexp with Golang",
    "section": "",
    "text": "DISCLAIMER: Let me say this first. Google’s regular expression implementations are known for not implementing features that make them, well, not regular. Both re2 and Golang’s regexp do not support backreferences. Otherwise, the things done here would be hard, or impossible.\nI may not be Golang’s biggest fan in general (lack of generics, verbose syntax, simplistic type system, etc), but I’ve written a bunch of it in the last couple of years and found an unexpectedly useful feature. Golang provides a package called regexp/syntax that proves to be as useful as its documentation is sparse.\nIn essence, this package exposes the user to the finite state machines built by the regexp compiler. This can be used to do analyses on regular expressions such as “does this regexp ever match a particular character after matching n characters?” or “does this regexp match any strings starting with a particular prefix?”. These might sound like constructed examples, but both of them actually popped up in my dayjob.\n\nToy Example: Loop detection\nFor sake of simplicity, let’s explore a constructed example first in this post: does a regexp match strings of arbitrary length? Or, in more technical terms: is there a loop in the finite state machine? Let’s get right started with the boilerplate of compiling a regular expression into something called a Prog.\nr, err := syntax.Parse(`.*`, syntax.Perl)\nif err != nil {\n    panic(fmt.Sprintf(\"Parse: %v\", err))\n}\n\np, err := syntax.Compile(r.Simplify())\nif err != nil {\n    panic(fmt.Sprintf(\"Compile: %v\", err))\n}\nfmt.Println(p)\nThis actually produces a nice and readable representation of the generated finite state machine. Note the * next to 2 which tells us that this is the initial state.\n  0 fail\n  1 anynotnl -&gt; 2\n  2*    alt -&gt; 1, 3\n  3 match\nWhat we see here is the textual representation of the Prog object. It’s a struct containing\n\nInst: list of instructions\nStart: initial instruction\n\nInst is the type used to represent an instruction. It’s a struct containing:\n\nOp: type of instruction\nOut: next instruction (except for InstMatch and InstFail, which are terminal instructions)\nArg: additional argument to some instructions\n\nFor the purposes of this toy example, all we care about is which instructions can follow from a given instruction. For most instructions, that is the instruction referred to in Out. Let’s introduce the odd ones here:\n\nInstMatch: successfully match input string. Does not have successor instruction.\nInstFail: reject input string. Does not have successor instruction.\nInstAlt / InstAltMatch: either Out or Arg are the successor instruction. A string matches if either of the branches arrives at InstMatch.\n\nIf you are curious about the difference between InstAlt and InstAltMatch: From all I could determine, InstAltMatch is an optimisation where it is known that one branch leads to a match while the other branch consumes characters. I do not see the compiler or any rewriting actually using this instruction, so it does not seem to be in use. Most of the implementation treats them interchangeably, while backtrack.go in the regex evaluator appears to use it to determine which branch to take.\nThis information allows us to implement a helper function to determining the successor instructions, given an instruction.\nfunc GetSuccessors(i *syntax.Inst) []uint32 {\n    if i.Op == syntax.InstMatch || i.Op == syntax.InstFail {\n        return []uint32{}\n    }\n    o := []uint32{i.Out}\n    if i.Op == syntax.InstAlt || i.Op == syntax.InstAltMatch {\n        o = append(o, i.Arg)\n    }\n    return o\n}\nNow we can implement loop-detection by a simple breadth-first search, keeping track of already visited nodes in a set (i.e. a map[uint32]bool, because Golang does not have a set type).\nfunc HasLoop(p *syntax.Prog) bool {\n    var tovisit []uint32\n    tovisit = append(tovisit, uint32(p.Start))\n    seen := make(map[uint32]bool)\n    for len(tovisit) != 0 {\n        i := tovisit[0]\n        if seen[i] {\n            return true\n        }\n        seen[i] = true\n        tovisit = tovisit[1:]\n        tovisit = append(tovisit, GetSuccessors(&p.Inst[i])...)\n    }\n    return false\n}\n\n\nA regex engine\nNow let’s try to build a very inefficient regex engine based on this. To do so, let us first introduce the various rune instructions (rune is Golang for Unicode codepoint). There is InstRune, InstRune1, InstRuneAny and InstRuneAnyNotNL. Most of them (except InstRune and InstRune1) should be self-explanatory, but here’s the whole list:\n\nInstRuneAny matches any rune.\nInstRuneAnyNotNL matches any rune except newlines.\nInstRune has a MatchRune method to determine whether it matches a rune.\nInstRune1 matches the rune provided in i.Rune[0] (obviously.)\n\nThis leaves us with two remaining useful instructions:\n\nInstCapture: capture a match into a capture group. We won’t bother with this here.\nInstEmptyWidth: match constrains on the current position in the string. This has a MatchEmptyWidth to determine whether it matches.\n\nThere’s also InstNop which, well, does nothing.\nOf course, the easiest way to do this is a recursive evaluator. We pass in a program, the current instruction, and input, and the current position in the input.\nfunc Match(p *syntax.Prog, pc uint32, input []rune, idx int) bool {\nLet’s first determine the previous and current rune at the current position, and use -1 for the borders of the string (to be consistent with MatchEmptyWidth).\n    var prev rune = -1\n    if idx &gt; 0 && idx &lt;= len(input) {\n        prev = input[idx-1]\n    }\n    var cur rune = -1\n    if idx &lt; len(input) {\n        cur = input[idx]\n    }\nAnd now all that’s left is one fairly large switch (if you look at actual implementations of regex engines, there are also often giant switches, so it’s legit).\nInstAlt and InstAltMatch are the same, so let’s use the fallthrough statement for go switch statements (this is much more sane than C-style fallthrough by default).\n    i := p.Inst[pc]\n    switch i.Op {\n    case syntax.InstAlt:\n        fallthrough\n    case syntax.InstAltMatch:\n        return Match(p, i.Out, input, idx) || Match(p, i.Arg, input, idx)\nWe don’t care about InstCapture or InstNop here.\n    case syntax.InstNop:\n        fallthrough\n    case syntax.InstCapture:\n        return Match(p, i.Out, input, idx)\nFor InstEmptyWidth we use the method that was given to us for this.\n    case syntax.InstEmptyWidth:\n        if !i.MatchEmptyWidth(prev, cur) {\n            return false\n        }\n        return Match(p, i.Out, input, idx)\n(I could get used to this). InstMatch and InstFail are obvious.\n    case syntax.InstMatch:\n        return true\n    case syntax.InstFail:\n        return false\nThen there come the various ways of matching runes. Note that this is the only time we have to increment the index into our input, as this is the only time we actually consume any runes.\n    case syntax.InstRune:\n        if cur == -1 || !i.MatchRune(cur) {\n            return false\n        }\n        return Match(p, i.Out, input, idx+1)\n    case syntax.InstRune1:\n        if cur == -1 || cur != i.Rune[0] {\n            return false\n        }\n        return Match(p, i.Out, input, idx+1)\n    case syntax.InstRuneAny:\n        if cur == -1 {\n            return false\n        }\n        return Match(p, i.Out, input, idx+1)\n    case syntax.InstRuneAnyNotNL:\n        if cur == -1 || cur == '\\n' {\n            return false\n        }\n        return Match(p, i.Out, input, idx+1)\nThat’s it. Now some due diligence against us being bad programmers, and we’re done.\n    default:\n        panic(\"Invalid instruction.\")\n    }\n    panic(\"Fell off the switch.\")\n}\nWell, that was fun. But not terribly exciting. But what this gives is as good mental model of what exactly the different instructions mean, which can be used to build more exciting things.\n\n\nDo we only match even-length strings?\nNow that we can proudly proclaim we have written a regular expression engine (well, maybe we were slightly cheating and someone else helped a bit), let’s take up a bigger challenge. Given a regular expression that possibly matches arbitrarily length strings, determine whether all strings matched have an even size. Sounds like an interview question? A bit, but I also hope I’ll never get this as an actual interview question.\nLet’s start with a similar prototype as for our Match function, but instead of our input let’s have something that flips around whether we are at an even or odd step. For reasons that will make sense later, let’s encode this as an integer that flips between 0 and 1 (so it’s idx % 2). We also need to keep track of which nodes we have seen before, or we will wait for a long time. But if you think about it a bit, we need to keep track of this for even and odd steps, an even visit and an odd visit are not the same. That results in the beautiful type of map[int]map[uint32]bool, or a map from an integer to a set of uint32.\nfunc Even(p *syntax.Prog, pc uint32, idx int, map[int]map[uint32]bool) bool {\nNow let’s start with the easy part. I literally copy & pasted the Match code, removed all the stuff that actually does any … matching, plumbed through the visited map and made idx mod 2. That gives us all the rune instructions and InstCapture, InstNop and InstEmptyWidth (which are all, for all intents and purposes, noop).\n    i := p.Inst[pc]\n    switch i.Op {\n    case syntax.InstNop:\n        fallthrough\n    case syntax.InstCapture:\n        fallthrough\n    case syntax.InstEmptyWidth:\n        return Even(p, i.Out, idx, visited)\n    case syntax.InstRune:\n        fallthrough\n    case syntax.InstRune1:\n        fallthrough\n    case syntax.InstRuneAny:\n        fallthrough\n    case syntax.InstRuneAnyNotNL:\n        return Even(p, i.Out, (idx+1)%2, visited)\nInstMatch is straightforward, as we just have to check whether we are at an even step. InstFail confusingly returns true, as we do not care about branches that do not lead to matches.\n    case syntax.InstMatch:\n        return idx == 0\n    case syntax.InstFail:\n        return true \nNow to one of my least favourite parts of Golang, copying nested maps. But here we go. Let’s introduce a helper method, as when we branch for InstAlt we will need a separate ropy of the visited map for both branches.\nfunc copyVisited(visited map[int]map[uint32]bool) map[int]map[uint32]bool {\n    n := make(map[int]map[uint32]bool)\n    for k, v := range visited {\n        if n[k] == nil {\n            n[k] = make(map[uint32]bool)\n        }\n        for k2, v2 := range v {\n            n[k][k2] = v2\n        }\n    }\n    return n\n}\nOK, with that out of the way, we can tackle the InstAlt instructions.\n    case syntax.InstAlt:\n        fallthrough\n    case syntax.InstAltMatch:\n        branchvisited := copyVisited(visited)\n        return Even(p, i.Out, idx, visited) && Even(p, i.Arg, idx, branchvisited)\nThis one is different, as we want to make sure we can only match even length strings, so we need to && the conditions, rather than || them.\nDue diligence again.\n    default:\n        panic(\"Invalid instruction.\")\n    }\n    panic(\"Fell off the switch.\")\nIf we stop for a second to see what we’ve built, it’s obvious we’ve built a potential infinite loop. Let’s rectify this. If we arrive back at an instruction that we’ve been before with the same idx % 2, we can just prune this branch and return true as it is exactly the same.\nSo let’s prepend our switch statement with the following.\n    if visited[idx][pc] {\n        return true\n    }\nAnd then, of course, we need to keep track of what we’ve seen before, which again involves one of my least favourite parts of Golang, nested maps.\n    if visited[idx] == nil {\n        visited[idx] = make(map[uint32]bool)\n    }\n    visited[idx][pc] = true\nAnd that’s done! To see the whole code used in this post head of to this Github Gist.\nIf you have made it this far, it is probably also worth noting that the Z3 SMT solver has a Regex Theory. The last time I’ve played with it it was giving obviously incorrect answers, but that seems to have been rectified since. The nice thing about regexp/syntax is that it uses the same library your application uses if you write it in Golang, and can be used in programs with more predictable performance and fewer dependencies than Z3."
  },
  {
    "objectID": "2016/04/17/a-paper-a-week-graphlab-framework-parallel-machine-learning.html",
    "href": "2016/04/17/a-paper-a-week-graphlab-framework-parallel-machine-learning.html",
    "title": "A Paper a Week-ish #14: GraphLab: A New Framework for Parallel Machine Learning",
    "section": "",
    "text": "It has been a long time since I last posted one of those, apologies! I have been fairly busy with various things and the time I had for this blog I spent on non paper-related posts to mix things up a bit.\nThe paper I read for this post is about GraphLab which is a framework for expressing parallel computations based on a graph model that allows to exploit the sparse structure of machine learning algorithms, that is to say that steps of the computation only operate on a subset of the state. In this graph data is associated with each vertex and edge, and a user-supplied update function is used to compute new values. This update function can only access and modify data in the neighbourhood of the node, that is the data of the node, its adjacent edges and vertices.\nIn addition to the data stored in the graph, there is a shared data table, which contains data which, as the name suggests, can be accessed by all applications of the update function – it can not be modified by them though. To modify data in the shared data table, a fold function that iteratively calculates a value from the data in the vertices and the previous value is specified by the user. Optionally, the user can scale the result of the fold or supply functions to merge results of different fold operations.\nBy imposing the restriction that the update function only operate on the neighbourhood of a node, the graph encodes the data dependencies between different parts of the computation and allows to determine which updates can be applied concurrently. GraphLab offers different consistency models: full consistency makes sure no data can be accessed concurrently, edge consistency ensures no two updates that access shared edges are executed at the same time, and vertex consistency only ensures only one update is applied to a particular node at a given time. From all I can see, vertex consistency only works correctly for trivially parallelizable problems, that is problems consisting of independent threads of computation that do not interact.\nA scheduler determines the sequence of sets of updates that should appear to be applied to the graph at the same time. So, for example, if the sequence is [\\(A\\), \\(B\\)] where \\(A\\) and \\(B\\) are sets of functions applied to vertices, it is ensured that (modulo the consistency model) it appears that all the operations in \\(A\\) were applied to the graph at the same time, then all operations in \\(B\\). Different machine learning algorithms require different schedulers.\nNote how I said “appear to be applied”, which means that this is not necessarily how they are actually applied. As an example, if vertex consistency is used, and \\(A = \\\\{v_1, v_2\\\\}\\) and \\(B = \\\\{v_3, v_4\\\\}\\), then \\(f(v_1)\\), \\(f(v_2)\\), \\(f(v_3)\\), \\(f(v_4)\\) can be evaluated at the same time, while it still appears that \\(B\\) was applied after \\(A\\). There is a very strong connection to how databases handle transactions – if the update function does not modify values that are not exclusive to it by the consistency model, we get serializability – that is, the parallel execution produces the same result as some sequential one. Database transactions are also designed to be serializable, and two of them can commit concurrently if they involve disjuct sets of rows.\nThe paper concludes with case studies of machine learning tasks implemented in GraphLab, which I will skip in this post.\nFind the paper here: GraphLab: A New Framework For Parallel Machine Learning."
  },
  {
    "objectID": "2016/10/14/operator-overloading.html",
    "href": "2016/10/14/operator-overloading.html",
    "title": "Fun with Operator Overloading",
    "section": "",
    "text": "The other day I was asked to take a look at code I had written years ago that basically allowed the user to build up logical formulas while always keeping them in CNF (conjunctive normal form – that is, a logical formula where the outermost connective is always a conjunction).\nThis was done by building a class hierarchy with an abstract class Expr on the top level, two classes ExprAnd and ExprOr that represent conjunction and disjunction respectively and both contain lists of Expr. Various other classes where used to represent various atoms of the formulas, the particular code was about solar physics so the atoms happen to be things like Wavelength(x, y) for selecting observations of a particular wavelength.\nThe base class Expr defines __and__ and __or__, that is the methods that need to be implemented to overload the & and | operations that just return ExprAnd and ExprOr of the object and the left hand operand.\nExprAnd and ExprOr also implement the operation corresponding to their node type by just returning a new ExprAnd or ExprOr with the left hand operand added to the list, respectively. They also implement __rand__ and __ror__, so their special implementation is also applied if they are used at the right hand side of the operation – or that was the plan. ExprAnd commutes | operations into the conjuction to keep conjunction as the top level connective.\nThe problem that I asked to help with was that foo & (bar & baz) did not seem to evaluate into ExprAnd(foo, bar, baz), but rather ExprAnd(foo, ExprAnd(bar, baz)), that is, the automatic transformation to CNF was not working.\nPython actually implements logic that when you have an operation foo & bar where the type of bar is a subclass of the type of foo and implements its own version of __rand__, the expression will be evaluating using that. So when you read __rand__ (or any of the operators) is used of the left hand side does not implement the operator, that is wrong. For instance,\nIn [18]: class Foo(object):\n    ...:     def __add__(self, x):\n    ...:         print \"Foo \", self, x\n    ...:         \n\nIn [19]: class Qux(Foo):\n    ...:     def __radd__(self, x):\n    ...:         print \"Qux \", self, x\n    ...:         \n\nIn [20]: Foo() + Qux()\nQux  &lt;__main__.Qux object at 0x112f9f750&gt; &lt;__main__.Foo object at 0x112f7e510&gt;\nThis behaviour was what I was aiming to exploit by making ExprAnd (for instance) implement both __and__ and __rand__ and make Python use the most specific implementation. Sadly, it does not work this way. If you now introduce a third class that inherits from Foo and does not implement either of the methods itself, the base class’ will be used, for instance\nIn [21]: class Bar(Foo):\n    ...:     pass\n    ...: \nIn [22]: Bar() + Qux()\nFoo  &lt;__main__.Bar object at 0x112f80890&gt; &lt;__main__.Qux object at 0x112f93a90&gt;\nThe semantics of Python operator overloading will only prefer the __rand__ method of the right hand side if it is an object of a subclass of the type of the right hand side object. Note that this does not take into account in which class the implementation of the left hand side was actually done, which explains the behaviour above.\nIf you look at the class layout described above, this was exactly the case that I was hitting. Disclaimer: What follows below is not how it was actually fixed in the code.\n\nGoing Crazy\nI realized that what I had intended to implement back then was something like commutative multimethod. Python’s dynamicity, for better or worse, gives you the tools to do most of such things, so I decided to give it a go.\nThe gist of the idea is to try to find the most specific implementation of the operation, considering both operands. The operation of the left hand side operand is more specific if it overrides the definition of the operation on the right hand side (and the other way round).\nThis can conveniently be expressed in terms of Python: The operation of the left hand side operand is more specific if the operation defined by the right hand side is in the MRO (method resolution order – the list of classes, in order, that get consulted when looking up a method in an object).\nTo implement this, first we define a functions to get all functions. It is important to know that the function is different to the unbound and bound method object and does not contain information about which class it is contained in; the function object can be obtained from an unbound method object by using im_func.\nimport inspect\n\ndef fn_mro(cls, fn):\n    mro = inspect.getmro(cls)\n    return [getattr(x, fn).im_func for x in mro if hasattr(x, fn)]\nThis can then be used to implement the logic as described above as a decorator that wraps a method. Note that we have to compare whether the left and right side operator is actually the same to prevent an infinite loop.\ndef most_specific(fn):\n    def func(self, other):\n        name = fn.__name__\n        self_fn = getattr(self, name).im_func\n        other_fn = getattr(other, name).im_func\n        if self_fn == other_fn:\n            return fn(self, other)\n        if self_fn not in fn_mro(other.__class__, name):\n            # We did not inherit this from something in other's MRO.\n            if other_fn not in fn_mro(self.__class__, name):\n                # They didn't either. PANIC!\n                raise TypeError\n            return fn(self, other)\n        else:\n            return getattr(other, name)(self)\n    return func\nIf we use the decorator on a slightly modified version of the previous example (we use __add__ instead of __radd__ because the decorator assumes commutativity and uses the same operations on both sides).\nIn [7]: class Foo(object):\n    @most_specific\n    def __add__(self, x):\n        print \"Foo \", self, x\n   ...:         \n\nIn [8]: class Qux(Foo):\n    @most_specific\n    def __add__(self, x):\n        print \"Qux \", self, x\n   ...:         \n\nIn [9]: Foo() + Qux()\nQux  &lt;__main__.Qux object at 0x103103690&gt; &lt;__main__.Foo object at 0x103103f50&gt;\n\nIn [10]: class Bar(Foo):\n   ....:     pass\n   ....: \n\nIn [11]: Bar() + Qux()\nQux  &lt;__main__.Qux object at 0x103103e50&gt; &lt;__main__.Bar object at 0x10310b050&gt;"
  },
  {
    "objectID": "2016/09/25/completeness.html",
    "href": "2016/09/25/completeness.html",
    "title": "Completeness and decidability",
    "section": "",
    "text": "The subtle difference between incompleteness (of a theory), completeness (of a proof system) and indecidability (of a logical system) are a common source of confusion. How come Gödel proved both completeness and incompleteness? How can a proof system be complete (there exists a proof for every true sentence) when the underlying logical system is undecidable (it is impossible to find a proof for every true sentence)?\nThe first question can be answered by something that should be familiar to everyone working in IT: bad naming. The “completeness” in the two theorems actually refers to two different things: completeness in the completeness theorem relates a formal proof system to model theory and says that a statement is true only if it can be proved (in a proof system for which completeness holds), that is \\(\\Pi \\models X \\Rightarrow \\Pi \\vdash X\\) – read \\(X\\) follows from \\(\\Pi\\) implies \\(X\\) is provable from \\(\\Pi\\); the completeness in the incompleteness theorem refers to whether a theory \\(\\Pi\\) contains every sentence or its negation, i.e. \\(\\forall X (\\Pi \\models X \\lor \\Pi \\models \\neg X)\\) – read for each \\(X\\), either \\(X\\) or \\(\\neg X\\) follow from \\(\\Pi\\). It is not hard to see that those two statements have no direct relationship to each other – one of them talks about a theory (the \\(\\Pi\\)), the other one about a proof system.\nThe second question is more subtle and can be explained by the slightly metaphysical definition of truth and existence in classical logic. We can rewrite the statement of the completeness theorem as “for each true statement, there exists a proof”. To do so, let’s define \\(P(x, \\Pi, \\phi)\\) to be the predicate that is true iff \\(x\\) is a valid proof for \\(\\Pi \\vdash \\phi\\). Then a formalisation would be \\(\\Pi \\models \\phi \\Rightarrow \\exists p\\;P(p, \\Pi, \\phi)\\). Decidability, which could be expressed as “there exists a procedure with which, for every true statement, we can find a proof”, on the other hand, could be expressed as (for the reader following at home – this definition works because f can just return an invalid proof for \\(\\Pi \\not\\models \\phi\\)) \\(\\exists f, P\\;( f \\text{ computable} \\land \\Pi \\models \\phi \\Leftrightarrow P(f(\\Pi, X), \\Pi, \\phi))\\).\nFor a sound proof system (one that only proves true sentences, i.e. \\(\\Pi \\models X \\Leftarrow \\Pi \\vdash X\\)), completeness of a proof system would imply decidabilty if you left out the part about computabilty. The difference is that explicitly quantifying over the function and requiring it be computable means there has to be a procedure, while a simple exists quantifier introduces an implicit function. The notion of procedure is a lot stronger than function; a function as defined in mathematics is simply a mapping from one set to the other (whereof there are uncountably infinite for infinite sets), while procedure requires a formal (finite) representation that could be executed by a computer (whereof there are countably infinite).\nThis mirrors the distinction between classical and intuitionistic logic. In classical logic, there is a platonic idea of truth (and thus existence): it is legitimate to refer to an object that one has no procedure on how to produce, if one can infer it has to exist (whatever it is). Intuitionistic logic, however, is a lot stricter and requires there to be a procedure on how to produce the objects."
  }
]